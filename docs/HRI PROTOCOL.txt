SERVIDOR => CLIENTE
Respuestas a prompts con ID: 
{
    type: "RESPONSE",
    data: {
        id: uuid,
        value: ...
    }
}
{ // El cliente envia un AUDIO_PROMPT y el servidor responde con la transcripcion mientras genera la respuesta
    type: "PROMPT_TRANSCRIPTION",
    data: {
        id: uuid,
        value: ...
    }
}
{ // Una vez ha enviado la respuesta y la transcripcion, cuando tiene el audio TTS, lo envia para que se reproduzca en el navegador
    type: "AUDIO_RESPONSE",
    data: {
        id: uuid,
        chunk_index: int,
        final: bool,
        audio: [int16],
        sample_rate: int
    }
}
Mensajes sobre la API de faceprints:
{
    "type": "FACEPRINT_EVENT",
    "data": {
        "event": "CREATE" | "DELETE" | "UPDATE",
        "name": name
    }
}

CLIENTE => SERVIDOR
Prompt del cliente:
{ // prompt de texto
    type: "PROMPT",
    data: {
        id: uuid,
        value: ...
    },
};
{ // prompt de audio
    type: "AUDIO_PROMPT_CHUNK",
    data: {
        id: uuid,
        chunk_index: int,
        final: bool,
        audio: [int16],
        sample_rate: int
    }
}

Se puede mejorar con streaming

ðŸŽ¯ Flujo propuesto:
Usuario graba y envÃ­a audio (por WebSocket o REST).
Servidor transcribe el audio con STT (e.g. Whisper).
EnvÃ­a PROMPT_TRANSCRIPTION â†’ el frontend muestra el mensaje del usuario.
Servidor llama al LLM para generar la respuesta textual.
EnvÃ­a RESPONSE â†’ el frontend muestra la respuesta textual del LLM.
Servidor sintetiza audio con TTS.
EnvÃ­a AUDIO_RESPONSE â†’ el frontend reproduce el audio automÃ¡ticamente.